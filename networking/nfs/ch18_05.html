<html>
<!-- Mirrored from nnc3.com/mags/Networking2/nfs/ch18_05.htm by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 28 Jul 2017 17:56:44 GMT -->
<head><title>NFS async thread tuning (Managing NFS and NIS, 2nd Edition)</title><link rel="stylesheet" type="text/css" href="../style/style1.css" />

<meta name="DC.Creator" content="Hal Stern, Mike Eisler and Ricardo Labiaga" /><meta name="DC.Format" content="text/xml" scheme="MIME" /><meta name="DC.Language" content="en-US" /><meta name="DC.Publisher" content="O'Reilly &amp; Associates, Inc." /><meta name="DC.Source" scheme="ISBN" content="1565925106L" /><meta name="DC.Subject.Keyword" content="stuff" /><meta name="DC.Title" content="Managing NFS and NIS, 2nd Edition" /><meta name="DC.Type" content="Text.Monograph" />

</head><body bgcolor="#ffffff">

<img alt="Book Home" border="0" src="gifs/smbanner.gif" usemap="#banner-map" /><map name="banner-map"><area shape="rect" coords="1,-2,616,66" href="index.html" alt="Managing NFS &amp; NIS" /><area shape="rect" coords="629,-11,726,25" href="jobjects/fsearch.html" alt="Search this book" /></map>

<div class="navbar"><table width="684" border="0"><tr><td align="left" valign="top" width="228"><a href="ch18_04.html"><img alt="Previous" border="0" src="../gifs/txtpreva.gif" /></a></td><td align="center" valign="top" width="228"><a href="index.html"></a></td><td align="right" valign="top" width="228"><a href="ch18_06.html"><img alt="Next" border="0" src="../gifs/txtnexta.gif" /></a></td></tr></table><div>



<h2 class="sect1">18.5. NFS async thread tuning</h2>

Early NFS client implementations<a name="INDEX-2695" />
provided <em class="emphasis">biod</em> user-level daemons in order to add
concurrency to NFS operations. In such implementations, a client
process performing an I/O operation on a file hands the request
<a name="INDEX-2696" /> <a name="INDEX-2697" />to
the <em class="emphasis">biod</em> daemon, and proceeds with its work
without blocking. The process doesn't have to wait for the I/O
request to be sent and acknowledged by the server, because the
<em class="emphasis">biod</em> daemon is responsible for issuing the
appropriate NFS operation request to the server and to wait for its
response. When the response is received, the
<em class="emphasis">biod</em> daemon is free to handle a new I/O request.
The idea is to have as many concurrent outstanding NFS operations as
the server can handle at once, in order to accelerate I/O handling.
Once all <em class="emphasis">biod</em> daemons are busy handling I/O
requests, the client-side process generating the requests has to
directly contact the NFS server and block awaiting its response.
</p>

For example, a file read request generated by the client-side process
is handed to one <em class="emphasis">biod</em> daemon, and the rest of
the <em class="emphasis">biod</em> daemons are asked to perform read-ahead
operations on the same file. The idea is to anticipate the next move
of the client-side application, by assuming that it is interested in
sequentially reading the file. The NFS client hopes to avoid having
to contact the NFS server on the next I/O request by the application,
by having the next chunk of data already available.
</p>

Solaris, as well as other modern Unix kernels support multiple
threads of execution without the need of a user context. Solaris has
no <em class="emphasis">biod</em> daemons, instead it uses kernel threads
to implement read-ahead and write-behind, achieving the same
increased read and write throughput.
</p>

The number of read-aheads performed once the Solaris client detects a
sequential read pattern is specified by the kernel tunable variables
<em class="emphasis">nfs_nra</em> for NFS Version 2 and
<em class="emphasis">nfs3_nra</em> for NFS Version 3. Solaris sets both
values to four read-aheads by default. Depending on your file access
patterns, network bandwidth, and hardware capabilities, you may need
to modify the number of read-aheads to achieve optimal use of your
resources. For example, you may find that this value needs to be
increased on Gigabit Ethernet, but decreased over ISDN. To reduce the
number of read-aheads over a low bandwidth connection, you can add
the following lines to <em class="emphasis">/etc/system</em> on the NFS
client and reboot the system:
</p>

<blockquote><pre class="code">set nfs:nfs_nra=2
set nfs:nfs3_nra=1</pre></blockquote>

When running over a high bandwidth network, make sure not to set
these values too high above their default, not only will sequential
read performance not improve, but the increased memory used by the
NFS async threads will ultimately degrade overall performance of the
system.
</p>

If <em class="emphasis">nfs3_nra</em> is set to four, and if you have two
processes<a name="INDEX-2698" /> reading two separate files concurrently
over NFSVersion 3, the system by default will generate four
read-aheads triggered by the read request of the first process, and
four more read-aheads triggered by the read request of the second
process for a total of eight concurrent read-aheads. The maximum
number of concurrent read-aheads for the entire system is limited by
the number of NFS async threads available. The kernel tunables
<em class="emphasis">nfs_max_threads</em> and
<em class="emphasis">nfs3_max_threads</em> control the maximum number of
active NFS async threads active at once per filesystem.
</p>

By default, a Solaris client uses eight NFS async threads per NFS
filesystem. To drop the number of NFS async threads to two, add the
following lines to <em class="emphasis">/etc/system</em> on the NFS client
and reboot the system:
</p>

<blockquote><pre class="code">set nfs:nfs_max_threads=2
set nfs:nfs3_max_threads=2</pre></blockquote>

After rebooting, you will have reduced the amount of NFS read-ahead
and write-behind performed by the client. Note that simply decreasing
the number of kernel threads may produce an effect similar to that of
eliminating them completely, so be conservative.
</p>

Be careful when server performance is a problem, since increasing NFS
async threads on the client machines beyond their default usually
makes the server performance problems worse. The NFS async threads
impose an implicit limit on the number of NFS requests requiring disk
I/O that may be outstanding from any client at any time. Each NFS
async thread has at most one NFS request outstanding at any time, and
if you increase the number of NFS async threads, you allow each
client to send more disk-bound requests at once, further loading the
network and the servers.
</p>

Decreasing the number of NFS async threads <a name="INDEX-2699" />doesn't always improve performance
either, and usually reduces NFS filesystem throughput. You must have
some small degree of NFS request multithreading on the NFS client to
maintain the illusion of having filesystem on local disks. Reducing
or eliminating the number of NFS async threads effectively throttles
the filesystem throughput of the NFS client  --  diminishing or
eliminating the amount of read-ahead and write-behind done.
</p>

In some cases, you may want to reduce write-behind client requests
because the network interface of the NFS server cannot handle that
many NFS write requests at once, such as when you have the NFS client
and NFS server on opposite sides of a 56-kbs connection. In these
radical cases, adequate performance can be achieved by reducing the
number of NFS async threads. Normally, an NFS async thread does
write-behind caching to improve NFS performance, and running multiple
NFS async threads allows a single process to have several write
requests outstanding at once. If you are running eight NFS async
threads on an NFS client, then the client will generate eight NFS
<em class="emphasis">write</em> requests at once when it is performing a
sequential write to a large file. The eight requests are handled by
the NFS async threads. In contrast to the <em class="emphasis">biod</em>
mechanism, when a Solaris process issues a new write requests while
all the NFS async threads are blocked waiting for a reply from the
server, the write request is queued in the kernel and the requesting
process returns successfully without blocking. The requesting process
does not issue an RPC to the NFS server itself, only the NFS async
threads do. When an NFS async thread RPC call completes, it proceeds
to grab the next request from the queue and sends a new RPC to the
server.
</p>

It may be necessary to reduce the number of NFS requests if a server
cannot keep pace with the incoming NFS <em class="emphasis">write</em>
requests. Reducing the number of NFS async threads accomplishes this;
the kernel RPC mechanism continues<a name="INDEX-2700" /> to work <a name="INDEX-2701" /> <a name="INDEX-2702" />without the async threads, albeit less
efficiently.
</p>



<hr width="684" align="left" />
<div class="navbar"><table width="684" border="0"><tr><td align="left" valign="top" width="228"><a href="ch18_04.html"><img alt="Previous" border="0" src="../gifs/txtpreva.gif" /></a></td><td align="center" valign="top" width="228"><a href="index.html"><img alt="Home" border="0" src="../gifs/txthome.gif" /></a></td><td align="right" valign="top" width="228"><a href="ch18_06.html"><img alt="Next" border="0" src="../gifs/txtnexta.gif" /></a></td></tr><tr><td align="left" valign="top" width="228">18.4. NFS over wide-area networks</td><td align="center" valign="top" width="228"><a href="index/index.html"><img alt="Book Index" border="0" src="../gifs/index.gif" /></a></td><td align="right" valign="top" width="228">18.6. Attribute caching</td></tr></table><div>
<hr width="684" align="left" />

<img alt="Library Navigation Links" border="0" src="../gifs/navbar.gif" usemap="#library-map" />
<p><font size="-1"><a href="copyrght.html">Copyright &copy; 2002</a> O'Reilly &amp; Associates. All rights reserved.</font></p>

<map name="library-map"><area shape="rect" coords="1,0,84,90" href="../index.html" /><area shape="rect" coords="86,-7,176,90" href="../ssh/index.html" /><area shape="rect" coords="178,0,265,101" href="../tcp/index.html" /><area shape="rect" coords="266,0,333,90" href="index.html" /><area shape="rect" coords="334,-1,429,93" href="../snmp/index.html" /><area shape="rect" coords="431,0,529,116" href="../tshoot/index.html" /><area shape="rect" coords="534,0,594,104" href="../dns/index.html" /><area shape="rect" coords="595,1,704,108" href="../fire/index-2.html" /></map>

</body>
<!-- Mirrored from nnc3.com/mags/Networking2/nfs/ch18_05.htm by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 28 Jul 2017 17:56:44 GMT -->
</html>
<html>
<!-- Mirrored from nnc3.com/mags/Networking2/nfs/ch17_03.htm by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 28 Jul 2017 17:56:43 GMT -->
<head><title>Network infrastructure (Managing NFS and NIS, 2nd Edition)</title><link rel="stylesheet" type="text/css" href="../style/style1.css" />

<meta name="DC.Creator" content="Hal Stern, Mike Eisler and Ricardo Labiaga" /><meta name="DC.Format" content="text/xml" scheme="MIME" /><meta name="DC.Language" content="en-US" /><meta name="DC.Publisher" content="O'Reilly &amp; Associates, Inc." /><meta name="DC.Source" scheme="ISBN" content="1565925106L" /><meta name="DC.Subject.Keyword" content="stuff" /><meta name="DC.Title" content="Managing NFS and NIS, 2nd Edition" /><meta name="DC.Type" content="Text.Monograph" />

</head><body bgcolor="#ffffff">

<img alt="Book Home" border="0" src="gifs/smbanner.gif" usemap="#banner-map" /><map name="banner-map"><area shape="rect" coords="1,-2,616,66" href="index.html" alt="Managing NFS &amp; NIS" /><area shape="rect" coords="629,-11,726,25" href="jobjects/fsearch.html" alt="Search this book" /></map>

<div class="navbar"><table width="684" border="0"><tr><td align="left" valign="top" width="228"><a href="ch17_02.html"><img alt="Previous" border="0" src="../gifs/txtpreva.gif" /></a></td><td align="center" valign="top" width="228"><a href="index.html"></a></td><td align="right" valign="top" width="228"><a href="ch17_04.html"><img alt="Next" border="0" src="../gifs/txtnexta.gif" /></a></td></tr></table><div>



<h2 class="sect1">17.3. Network infrastructure</h2>


Partitioning a low-bandwidth network should <a name="INDEX-2593" />
<a name="INDEX-2594" />ease the
<a name="INDEX-2595" /> <a name="INDEX-2596" />
<a name="INDEX-2597" />constraints imposed by the network on
attribute-intensive applications, but may not necessarily address the
limitations encountered by data-intensive applications.
Data-intensive applications require high bandwidth, and may require
the hosts to be migrated onto higher bandwidth networks, such as Fast
Ethernet, FDDI, ATM, or Gigabit Ethernet. Recent advances in
networking as well as economies of scale have made high bandwidth and
switched networks more accessible. We explore their effects on NIS
and NFS in the remaining sections of this chapter.</p>


<a name="nfs2-CHP-17-SECT-3.1" /><div class="sect2">
<h3 class="sect2">17.3.1. Switched networks</h3>


Switched Ethernets have become<a name="INDEX-2598" /> <a name="INDEX-2599" /> affordable and extremely popular in
the last few years, with configurations ranging from enterprise-class
switching networks with hundreds of ports, to the small 8- and
16-port Fast Ethernet switched networks used in small businesses.
Switched Ethernets are commonly found in configurations that use a
high-bandwidth interface into the server (such as Gigabit Ethernet)
and a switching hub that distributes the single fast network into a
large number of slower branches (such as Fast Ethernet ports). This
topology isolates a client's traffic to the server from the
other clients on the network, since each client is on a different
branch of the network. This reduces the collision rate, allowing each
client to utilize higher bandwidth when communicating to the server.</p>


Although switched networks alleviate the impact of collisions, you
still have to watch for "impedance mismatches" between an
excessive number of client network segments and only a few server
segments. A typical problem in a switched network environment occurs
when an excessive number of NFS clients capable of saturating their
own network segments overload the server's "narrow"
network segment.</p>


Consider the case where 100 NFS clients and a single NFS server are
all connected to a switched Fast Ethernet. The server and each of its
clients have their own 100 Mbit/sec port on the switch. In this
configuration, the server can easily become bandwidth starved when
multiple concurrent requests from the NFS clients arrive over its
single network segment. To address this problem, you should provide
multiple network interfaces to the server, each connected to its own
100 Mb/sec port on the switch. You can either turn on IP interface
groups on the server, such that the server can have more than one IP
address on the same subnet, or use the outbound networks for
multiplexing out the NFS read replies. The clients should use all of
the hosts' IP addresses in order for the inbound requests to
arrive over the various network interfaces. You can
<a name="INDEX-2600" />configure BIND
round-robin<a href="#FOOTNOTE-52">[52]</a>
if you don't want to hardcode the destination addresses. You
can alternatively enable interface trunking on the server to use the
multiple network interfaces as a single IP address avoiding the need
to mess with IP addressing and client naming conventions.
Trunking<a name="INDEX-2601" /> also
offers a measure of fault tolerance, since the trunked interface
keeps working even if one of the network interfaces fails. Finally,
trunking scales as you add more network interfaces to the server,
providing additional network bandwidth. Many switches provide a
combination of Fast Ethernet and Gigabit Ethernet channels as well.
They can also support the aggregation of these channels to provide
high bandwidth to either data center servers or to the backbone
network.</p><blockquote class="footnote">

<a name="FOOTNOTE-52" />[52]When BIND's round-robin feature is
enabled, the order of the server's addresses returned is
shifted on each query to the name server. This allows a different
address to be used by each client's request.</p>

</blockquote>


Heavily used NFS servers will benefit from their own
"fast" branch, but try to keep NFS clients and servers
logically close in the network topology. Try to minimize the number
of switches and routers that traffic must cross. A good rule of thumb
is to try to keep 80% of the traffic within the network and only 20%
of the<a name="INDEX-2602" />
<a name="INDEX-2603" /> traffic
from accessing the backbone.</p>
</div>




<a name="nfs2-CHP-17-SECT-3.2" /><div class="sect2">
<h3 class="sect2">17.3.2. ATM and FDDI networks</h3>


ATM (Asynchronous Transfer Mode) and FDDI (Fiber Distributed Data
Interface) networks are two other forms<a name="INDEX-2604" />
<a name="INDEX-2605" />
<a name="INDEX-2606" />
<a name="INDEX-2607" />
of high-bandwidth networks that can sustain multiple high-speed
concurrent data exchanges with minimal degradation. ATM and FDDI are
somewhat more efficient than Fast Ethernet in data-intensive
environments because they use a larger MTU (Maximum Transfer Unit),
therefore requiring less packets than Fast Ethernet to transmit the
same amount of information. Note that this does not necessarily
present an advantage to attribute-intensive environments where the
requests are small and always fit in a Fast Ethernet packet.</p>


Although ATM promises scalable and seamless bandwidth, guaranteed QoS
(Quality of Service), integrated services (voice, video, and data),
and virtual networking, Ethernet technologies are not likely to be
displaced. Today, ATM has not been widely deployed outside backbone
networks. Many network administrators prefer to deploy Fast Ethernet
and Gigabit Ethernet because of their familiarity with the protocol,
and because it requires no changes to the packet format. This means
that existing analysis and network management tools and software that
operate at the network and transport layers, and higher, continue to
work as before. It is unlikely that ATM will experience <a name="INDEX-2608" /> <a name="INDEX-2609" />a significant amount
of deployment outside the <a name="INDEX-2610" /> <a name="INDEX-2611" /> <a name="INDEX-2612" /> <a name="INDEX-2613" />backbone.</p>
</div>


<hr width="684" align="left" />
<div class="navbar"><table width="684" border="0"><tr><td align="left" valign="top" width="228"><a href="ch17_02.html"><img alt="Previous" border="0" src="../gifs/txtpreva.gif" /></a></td><td align="center" valign="top" width="228"><a href="index.html"><img alt="Home" border="0" src="../gifs/txthome.gif" /></a></td><td align="right" valign="top" width="228"><a href="ch17_04.html"><img alt="Next" border="0" src="../gifs/txtnexta.gif" /></a></td></tr><tr><td align="left" valign="top" width="228">17.2. Network partitioning hardware</td><td align="center" valign="top" width="228"><a href="index/index.html"><img alt="Book Index" border="0" src="../gifs/index.gif" /></a></td><td align="right" valign="top" width="228">17.4. Impact of partitioning</td></tr></table><div>
<hr width="684" align="left" />

<img alt="Library Navigation Links" border="0" src="../gifs/navbar.gif" usemap="#library-map" />
<p><font size="-1"><a href="copyrght.html">Copyright &copy; 2002</a> O'Reilly &amp; Associates. All rights reserved.</font></p>

<map name="library-map"><area shape="rect" coords="1,0,84,90" href="../index.html" /><area shape="rect" coords="86,-7,176,90" href="../ssh/index.html" /><area shape="rect" coords="178,0,265,101" href="../tcp/index.html" /><area shape="rect" coords="266,0,333,90" href="index.html" /><area shape="rect" coords="334,-1,429,93" href="../snmp/index.html" /><area shape="rect" coords="431,0,529,116" href="../tshoot/index.html" /><area shape="rect" coords="534,0,594,104" href="../dns/index.html" /><area shape="rect" coords="595,1,704,108" href="../fire/index-2.html" /></map>

</body>
<!-- Mirrored from nnc3.com/mags/Networking2/nfs/ch17_03.htm by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 28 Jul 2017 17:56:43 GMT -->
</html>